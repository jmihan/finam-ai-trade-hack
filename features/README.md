Пайплайн состоит из следующих основных шагов:
1.  **Загрузка и подготовка данных:** Исходные данные (свечи для получения тикеров и новостные тексты) загружаются и предварительно обрабатываются.
2.  **Сопоставление тикеров с новостями:** Для каждой новости определяются релевантные тикеры компаний путем поиска точных совпадений в тексте.
3.  **Извлечение количественных признаков:** Расчитываются базовые текстовые метрики (количество символов, слов, средняя длина слова, количество заглавных слов, повторяющихся слов, специальных символов).
4.  **Извлечение TinyBERT эмбеддингов:** Используется предобученная модель TinyBERT для получения векторных представлений новостных текстов (Mean Pooling из последнего скрытого слоя).
5.  **Извлечение EmoBERT признаков:** Применяется предобученная модель EmoBERT для определения вероятностей различных эмоциональных категорий в новостях.
6.  **Объединение и разворачивание данных:** Все извлеченные признаки объединяются с исходными новостями и "разворачиваются" таким образом, что каждая запись соответствует паре (новость, связанный тикер).
7.  **Агрегация признаков:** Признаки группируются по датам и тикерам. Для количественных признаков используется `mean` или `sum`, для эмбеддингов и вероятностей эмоций — `mean`. Также подсчитывается количество новостей за день для каждого тикера.
8.  **Сохранение результатов:** Финальные агрегированные признаки сохраняются в формате Parquet.



Описание Файлов

*   **`main.py`**: Основной исполняемый скрипт, который оркестрирует весь пайплайн. Вызывает функции из других модулей в нужной последовательности.
*   **`config.py`**: Содержит все конфигурационные параметры проекта: пути к данным, имена файлов кэша, пути к моделям, настройки устройства (CPU/GPU) и батчей.
*   **`utils.py`**: Вспомогательные функции, такие как загрузка и сохранение кэша в формате Parquet.
*   **`data_loader.py`**: Отвечает за загрузку исходных CSV-файлов, предварительную обработку данных и сопоставление новостей с тикерами.
*   **`feature_extractor.py`**: Реализует логику извлечения всех признаков: количественных, TinyBERT эмбеддингов и EmoBERT вероятностей. Использует кэширование для ускорения повторных запусков.
*   **`aggregator.py`**: Объединяет все извлеченные признаки, обрабатывает их для каждой пары (новость, тикер) и агрегирует их по дням и тикерам.


NLP-модели
    *   Скрипты используют локально скачанные модели: TinyBERT (`prajjwal1/bert-tiny`) и EmoBERT (`cointegrated/rubert-tiny2-cedr-emotion-detection`).
    *   Необходимо скачать эти модели локально и сохранить их в директории, указанной в `features/config.py` как `MODELS_DIR` (по умолчанию `C:/Users/nikit/Desktop/models`).
    *   Модели должны быть сохранены в подпапках `bert-tiny` и `emobert` соответственно.


Настройка `config.py`

*   Откройте `features/config.py`.
*   Убедитесь, что `TRAIN_CANDLES_PATH` (`../train_candles.csv`) и `TEST_NEWS_PATH` (`../test_news.csv`) корректно указывают на данные в корне проекта.
*   **Укажите правильный абсолютный путь к директории с моделями в `MODELS_DIR`**.
*   При необходимости отрегулируйте `BATCH_SIZE` (по умолчанию `4`). 
