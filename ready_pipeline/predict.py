import pandas as pd
import numpy as np
import logging
import sys
import os
import joblib
import torch
from transformers import PatchTSTForPrediction, Trainer, TrainingArguments
from tsfm_public.toolkit.dataset import ForecastDFDataset

# --- –ë–ª–æ–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ ---
try:
    from config import *
except ImportError:
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from config import *
# --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ---

def run_prediction():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    """
    logging.info("="*50)
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    logging.info("="*50)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    logging.info("--- –®–∞–≥ 1/4: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ ---")
    try:
        model = PatchTSTForPrediction.from_pretrained(PATCHTST_MODEL_PATH).to(DEVICE)
        model.eval()
        
        artifacts = joblib.load(PATCHTST_PREPROCESSOR_PATH)
        preprocessor = artifacts['preprocessor']
        context_scalers = artifacts['context_scalers']
        
        logging.info("–ú–æ–¥–µ–ª—å, –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏ —Å–∫–µ–π–ª–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}.")
        return

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logging.info("--- –®–∞–≥ 2/4: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---")
    try:
        df_train = pd.read_parquet(FINAL_TRAIN_DF_PATH)
        df_inference_tail = pd.read_parquet(INFERENCE_DF_PATH)
        df_full = pd.concat([df_train, df_inference_tail]).sort_values(['ticker', 'date'])
    except FileNotFoundError as e:
        logging.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏: {e}. –ó–∞–ø—É—Å—Ç–∏—Ç–µ prepare_final_dataset.py.")
        return
        
    inference_chunks = [df_full[df_full['ticker'] == ticker].tail(CONTEXT_LENGTH) for ticker in df_full['ticker'].unique()]
    df_inference = pd.concat(inference_chunks)

    target_columns = [f'target_return_{i}d' for i in range(1, PREDICTION_LENGTH + 1)]
    reserved_cols = ['date', 'ticker'] + target_columns
    context_columns = [col for col in df_full.columns if col not in reserved_cols]
    df_inference[target_columns] = df_inference[target_columns].fillna(0.0)

    # 3. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    logging.info("--- –®–∞–≥ 3/4: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ---")
    for ticker in df_inference['ticker'].unique():
        if ticker in context_scalers:
            mask = df_inference['ticker'] == ticker
            df_inference.loc[mask, context_columns] = context_scalers[ticker].transform(df_inference.loc[mask, context_columns])
            
    processed_inference_df = preprocessor.preprocess(df_inference)
    inference_dataset = ForecastDFDataset(processed_inference_df, id_columns=["ticker"], timestamp_column="date",
                                          target_columns=target_columns, conditional_columns=context_columns,
                                          context_length=CONTEXT_LENGTH, prediction_length=PREDICTION_LENGTH)

    trainer = Trainer(model=model, args=TrainingArguments(output_dir="./temp_output", per_device_eval_batch_size=BATCH_SIZE))
    predictions_output = trainer.predict(inference_dataset)
    
    # 4. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    logging.info("--- –®–∞–≥ 4/4: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---")
    
    predictions_for_all_channels = predictions_output.predictions[1]
    predictions_raw = predictions_for_all_channels[:, :, :PREDICTION_LENGTH]

    # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –î–õ–Ø –°–û–ó–î–ê–ù–ò–Ø SUBMISSION –° –£–ß–ï–¢–û–ú –í–´–•–û–î–ù–´–• ---
    
    last_known_date = df_full['date'].max()
    tickers_in_order = sorted(df_inference['ticker'].unique())

    # 1. –°–æ–∑–¥–∞–µ–º –¥–≤–∞ —Å–ø–∏—Å–∫–∞ –¥–∞—Ç: 20 –ö–ê–õ–ï–ù–î–ê–†–ù–´–• –¥–Ω–µ–π –∏ 20 –¢–û–†–ì–û–í–´–• –¥–Ω–µ–π
    calendar_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1), periods=PREDICTION_LENGTH, freq='D')
    trading_dates = pd.bdate_range(start=last_known_date + pd.Timedelta(days=1), periods=PREDICTION_LENGTH)
    
    results_list = []
    for i, ticker in enumerate(tickers_in_order):
        prediction_matrix_for_ticker = predictions_raw[i]
        diagonal_predictions = np.diag(prediction_matrix_for_ticker) # –í–µ–∫—Ç–æ—Ä –∏–∑ 20 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –¢–û–†–ì–û–í–´–• –¥–Ω–µ–π

        # 2. –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å: {—Ç–æ—Ä–≥–æ–≤–∞—è_–¥–∞—Ç–∞: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ}
        trading_predictions_map = dict(zip(trading_dates, diagonal_predictions))
        
        # 3. –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –ö–ê–õ–ï–ù–î–ê–†–ù–´–ú –¥–Ω—è–º
        ticker_calendar_preds = {'ticker': ticker}
        for day_num, calendar_date in enumerate(calendar_dates):
            p_col = f'p{day_num + 1}'
            
            # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–π –¥–µ–Ω—å —Ç–æ—Ä–≥–æ–≤—ã–º.
            # –ï—Å–ª–∏ –¥–∞ - –±–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–∑ —Å–ª–æ–≤–∞—Ä—è. –ï—Å–ª–∏ –Ω–µ—Ç (–≤—ã—Ö–æ–¥–Ω–æ–π) - —Å—Ç–∞–≤–∏–º 0.
            prediction_value = trading_predictions_map.get(calendar_date, 0.0)
            ticker_calendar_preds[p_col] = prediction_value
        
        results_list.append(ticker_calendar_preds)

    # 5. –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π DataFrame
    df_submission = pd.DataFrame(results_list)
    
    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    final_columns = ['ticker'] + [f'p{i}' for i in range(1, PREDICTION_LENGTH + 1)]
    df_submission = df_submission[final_columns]
    
    # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª
    df_submission.to_csv(SUBMISSION_PATH, index=False)
    
    logging.info(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π submission-—Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {SUBMISSION_PATH}")
    logging.info(f"–ü—Ä–∏–º–µ—Ä submission-—Ñ–∞–π–ª–∞ (—Å –Ω—É–ª—è–º–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö):\n{df_submission.head().to_string()}")


if __name__ == '__main__':
    run_prediction()