import pandas as pd
import numpy as np
import logging
import sys
import os
import joblib
from sklearn.preprocessing import StandardScaler

# --- –ò–º–ø–æ—Ä—Ç—ã –∏–∑ Hugging Face –∏ tsfm ---
from transformers import (
    PatchTSTConfig,
    PatchTSTForPrediction,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    set_seed,
)
from tsfm_public.toolkit.dataset import ForecastDFDataset
from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor

# --- –ë–ª–æ–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ ---
try:
    from config import *
except ImportError:
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from config import *
# --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ---

set_seed(42)

def run_model_training():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ PatchTST.
    """
    logging.info("="*50)
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ PatchTST...")
    logging.info("="*50)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    logging.info("--- –®–∞–≥ 1/6: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ---")
    try:
        df = pd.read_parquet(FINAL_TRAIN_DF_PATH)
        df['date'] = pd.to_datetime(df['date'])
    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª {FINAL_TRAIN_DF_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
    logging.info("--- –®–∞–≥ 2/6: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ç–∞—Ä–≥–µ—Ç–æ–≤ ---")
    target_columns = [f'target_return_{i}d' for i in range(1, PREDICTION_LENGTH + 1)]
    reserved_cols = ['date', 'ticker'] + target_columns
    context_columns = [col for col in df.columns if col not in reserved_cols]
    
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(context_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (context_columns).")
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(target_columns)} —Ç–∞—Ä–≥–µ—Ç–æ–≤ (target_columns).")

    # 3. –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/validation
    logging.info("--- –®–∞–≥ 3/6: –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ ---")
    unique_dates = sorted(df['date'].unique())
    split_date = unique_dates[int(len(unique_dates) * 0.8)]
    train_df = df[df['date'] < split_date]
    val_df = df[df['date'] >= split_date]
    logging.info(f"–û–±—É—á–µ–Ω–∏–µ: {len(train_df)} —Å—Ç—Ä–æ–∫. –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(val_df)} —Å—Ç—Ä–æ–∫.")

    # 4. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    logging.info("--- –®–∞–≥ 4/6: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ PyTorch –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ---")
    
    def scale_per_ticker(df, columns, fit=False, scalers=None):
        if scalers is None:
            scalers = {}
        for ticker in df['ticker'].unique():
            mask = df['ticker'] == ticker
            if fit:
                scalers[ticker] = StandardScaler().fit(df.loc[mask, columns])
            df.loc[mask, columns] = scalers[ticker].transform(df.loc[mask, columns])
        return df, scalers

    train_df, context_scalers = scale_per_ticker(train_df, context_columns, fit=True)
    val_df, _ = scale_per_ticker(val_df, context_columns, scalers=context_scalers)

    # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –¢–û–õ–¨–ö–û –ø—Ä–∏–∑–Ω–∞–∫–∏ (context_columns)
    preprocessor = TimeSeriesPreprocessor(
        timestamp_column="date",
        id_columns=["ticker"],
        input_columns=context_columns,
        target_columns=target_columns,
        scaling=True
    )
    preprocessor.train(train_df)
    
    # --- –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ë–õ–û–ö –°–û–ì–õ–ê–°–ù–û HELP() ---
    train_dataset = ForecastDFDataset(
        preprocessor.preprocess(train_df),
        id_columns=["ticker"],
        timestamp_column="date",
        target_columns=target_columns,       # <--- –ó–¥–µ—Å—å –¢–û–õ–¨–ö–û —Ç–∞—Ä–≥–µ—Ç—ã
        conditional_columns=context_columns, # <--- –ó–¥–µ—Å—å –¢–û–õ–¨–ö–û –ø—Ä–∏–∑–Ω–∞–∫–∏
        context_length=CONTEXT_LENGTH,
        prediction_length=PREDICTION_LENGTH,
    )
    val_dataset = ForecastDFDataset(
        preprocessor.preprocess(val_df),
        id_columns=["ticker"],
        timestamp_column="date",
        target_columns=target_columns,       # <--- –ó–¥–µ—Å—å –¢–û–õ–¨–ö–û —Ç–∞—Ä–≥–µ—Ç—ã
        conditional_columns=context_columns, # <--- –ó–¥–µ—Å—å –¢–û–õ–¨–ö–û –ø—Ä–∏–∑–Ω–∞–∫–∏
        context_length=CONTEXT_LENGTH,
        prediction_length=PREDICTION_LENGTH,
    )
    # --- –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô ---
    logging.info("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã.")

    # 5. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ Trainer'–∞
    logging.info("--- –®–∞–≥ 5/6: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ Trainer ---")
    
    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–∞—Å—Ç –≤ –º–æ–¥–µ–ª—å
    num_total_channels = len(target_columns) + len(context_columns)
    
    config = PatchTSTConfig(
        # –ú–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤
        num_input_channels=num_total_channels, # <--- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        context_length=CONTEXT_LENGTH,
        prediction_length=PREDICTION_LENGTH,
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Ç–∞—Ä–≥–µ—Ç–∞–º–∏. –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∏—Ö.
        num_output_channels=len(target_columns), # <--- –í–∞–∂–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        patch_length=PATCH_LENGTH,
        patch_stride=PATCH_LENGTH,
        d_model=D_MODEL,
        num_attention_heads=N_HEADS,
        num_hidden_layers=ENCODER_LAYERS,
        ffn_dim=D_MODEL * 2,
        dropout=DROPOUT,
        head_dropout=DROPOUT,
        loss="mse",
    )
    model = PatchTSTForPrediction(config)

    training_args = TrainingArguments(
        output_dir=PATCHTST_MODEL_PATH.parent / "checkpoints",
        overwrite_output_dir=True,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        do_eval=True,
        evaluation_strategy="epoch", 
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        fp16=True,
        dataloader_num_workers=4,
        save_strategy="epoch",
        save_total_limit=1,
        logging_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="none",
        label_names=["future_values"],
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)],
    )
    logging.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

    # 6. –û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    trainer.train()
    
    logging.info("\n--- –®–∞–≥ 6/6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ ---")
    results = trainer.evaluate()
    logging.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (MSE): {results['eval_loss']:.6f}")

    trainer.save_model(PATCHTST_MODEL_PATH)
    artifacts_to_save = {
        'preprocessor': preprocessor,
        'context_scalers': context_scalers
    }
    joblib.dump(artifacts_to_save, PATCHTST_PREPROCESSOR_PATH)

    logging.info(f" –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {PATCHTST_MODEL_PATH}")
    logging.info(f" –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {PATCHTST_PREPROCESSOR_PATH}")
    logging.info("\n–ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == '__main__':
    run_model_training()